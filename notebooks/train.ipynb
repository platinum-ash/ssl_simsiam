{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd6f5907",
   "metadata": {},
   "source": [
    "### Section 1: Imports and Setup\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22f27f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# SimSiam Training on SSL4EO-S12 Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d2c2e4",
   "metadata": {},
   "source": [
    "### Section 2: Data Augmentation Pipeline\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a9d8c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimSiamTransforms:\n",
    "    \"\"\"\n",
    "    Data augmentation pipeline for SimSiam following the original paper\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224):\n",
    "        # Strong augmentation pipeline\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((img_size, img_size)),\n",
    "            transforms.RandomResizedCrop(img_size, scale=(0.2, 1.0)),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomVerticalFlip(p=0.5),  # Common for satellite imagery\n",
    "            transforms.RandomRotation(degrees=90),  # 90-degree rotations for satellite data\n",
    "            transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n",
    "            transforms.RandomGrayscale(p=0.2),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.transform(x), self.transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d859199",
   "metadata": {},
   "source": [
    "\n",
    "### Section 3: SSL4EO-S12 Dataset Class\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0a116dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import SSL4EO_S12_Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1b5c11",
   "metadata": {},
   "source": [
    "### Section 4: SimSiam Model Implementation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12561e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionMLP(nn.Module):\n",
    "    \"\"\"Projection MLP for SimSiam\"\"\"\n",
    "    def __init__(self, input_dim=2048, hidden_dim=2048, output_dim=2048):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "            nn.BatchNorm1d(output_dim, affine=False)  # No bias/scale in final BN\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        return x\n",
    "\n",
    "class PredictionMLP(nn.Module):\n",
    "    \"\"\"Prediction MLP for SimSiam\"\"\"\n",
    "    def __init__(self, input_dim=2048, hidden_dim=512, output_dim=2048):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.layer2 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "\n",
    "class SimSiam(nn.Module):\n",
    "    \"\"\"\n",
    "    SimSiam model implementation\n",
    "    \"\"\"\n",
    "    def __init__(self, backbone='resnet50', proj_dim=2048, pred_dim=512):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Backbone encoder\n",
    "        if backbone == 'resnet50':\n",
    "            self.encoder = models.resnet50(pretrained=False)\n",
    "            self.encoder.fc = nn.Identity()  # Remove classification head\n",
    "            encoder_dim = 2048\n",
    "        elif backbone == 'resnet18':\n",
    "            self.encoder = models.resnet18(pretrained=False)\n",
    "            self.encoder.fc = nn.Identity()\n",
    "            encoder_dim = 512\n",
    "        else:\n",
    "            raise ValueError(f\"Backbone {backbone} not supported\")\n",
    "        \n",
    "        # Projection head\n",
    "        self.projector = ProjectionMLP(encoder_dim, proj_dim, proj_dim)\n",
    "        \n",
    "        # Prediction head\n",
    "        self.predictor = PredictionMLP(proj_dim, pred_dim, proj_dim)\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        # Encode both views\n",
    "        z1 = self.projector(self.encoder(x1))\n",
    "        z2 = self.projector(self.encoder(x2))\n",
    "        \n",
    "        # Predict\n",
    "        p1 = self.predictor(z1)\n",
    "        p2 = self.predictor(z2)\n",
    "        \n",
    "        return p1, p2, z1.detach(), z2.detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab79cd9",
   "metadata": {},
   "source": [
    "### Section 5: Loss Function\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93335708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simsiam_loss(p1, p2, z1, z2):\n",
    "    \"\"\"\n",
    "    SimSiam loss function\n",
    "    Negative cosine similarity\n",
    "    \"\"\"\n",
    "    def cosine_similarity(a, b):\n",
    "        a = F.normalize(a, dim=1)\n",
    "        b = F.normalize(b, dim=1)\n",
    "        return (a * b).sum(dim=1).mean()\n",
    "    \n",
    "    loss1 = -cosine_similarity(p1, z2)\n",
    "    loss2 = -cosine_similarity(p2, z1)\n",
    "    \n",
    "    return (loss1 + loss2) * 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b684f6a3",
   "metadata": {},
   "source": [
    "\n",
    "### Section 6: Training Configuration\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad77306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "config = {\n",
    "    'batch_size': 64,\n",
    "    'learning_rate': 0.05,\n",
    "    'weight_decay': 1e-4,\n",
    "    'epochs': 10,\n",
    "    'img_size': 224,\n",
    "    'backbone': 'resnet18',\n",
    "    'data_dir': './temp_zarr', \n",
    "    'save_dir': './checkpoints_new_backbone',\n",
    "    'log_interval': 10,\n",
    "    'run': 'Run_batch_size_64_epoch_10_resnet_18_new_backbone'\n",
    "}\n",
    "\n",
    "# Create save directory\n",
    "os.makedirs(config['save_dir'], exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21630cb",
   "metadata": {},
   "source": [
    "### Section 7: Data Loading\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1c2337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize transforms and dataset\n",
    "transform = SimSiamTransforms(img_size=config['img_size'])\n",
    "\n",
    "# Update the data_dir path to your SSL4EO-S12 dataset location\n",
    "dataset = SSL4EO_S12_Dataset(\n",
    "    extracted_dir=config['data_dir'],\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Create data loader\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "print(f\"Number of batches: {len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badfc682",
   "metadata": {},
   "source": [
    "### Section 8: Model Initialization\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a98ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = SimSiam(backbone=config['backbone']).to(device)\n",
    "\n",
    "# Initialize optimizer with cosine annealing\n",
    "optimizer = optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=config['learning_rate'],\n",
    "    momentum=0.9,\n",
    "    weight_decay=config['weight_decay']\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=config['epochs']\n",
    ")\n",
    "\n",
    "print(f\"Model initialized with {sum(p.numel() for p in model.parameters()):,} parameters\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a54b6f3",
   "metadata": {},
   "source": [
    "### Section 9: Training Loop\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac68ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 1524/1524 [29:08<00:00,  1.15s/it, Iter=1520, Loss=-0.8624, Avg Loss=-0.8082, LR=0.050000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Average Loss: -0.8083, LR: 0.048776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/10: 100%|██████████| 1524/1524 [28:57<00:00,  1.14s/it, Iter=3040, Loss=-0.8860, Avg Loss=-0.8714, LR=0.048776]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Average Loss: -0.8714, LR: 0.045225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/10: 100%|██████████| 1524/1524 [28:51<00:00,  1.14s/it, Iter=4570, Loss=-0.8833, Avg Loss=-0.8757, LR=0.045225]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Average Loss: -0.8757, LR: 0.039695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/10: 100%|██████████| 1524/1524 [28:46<00:00,  1.13s/it, Iter=6090, Loss=-0.8899, Avg Loss=-0.8775, LR=0.039695]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Average Loss: -0.8775, LR: 0.032725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/10: 100%|██████████| 1524/1524 [28:52<00:00,  1.14s/it, Iter=7610, Loss=-0.8695, Avg Loss=-0.8861, LR=0.032725]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Average Loss: -0.8861, LR: 0.025000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/10: 100%|██████████| 1524/1524 [28:43<00:00,  1.13s/it, Iter=9140, Loss=-0.9087, Avg Loss=-0.8942, LR=0.025000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Average Loss: -0.8942, LR: 0.017275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/10: 100%|██████████| 1524/1524 [28:46<00:00,  1.13s/it, Iter=10660, Loss=-0.9138, Avg Loss=-0.8990, LR=0.017275]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Average Loss: -0.8990, LR: 0.010305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/10: 100%|██████████| 1524/1524 [28:44<00:00,  1.13s/it, Iter=12190, Loss=-0.9266, Avg Loss=-0.9031, LR=0.010305]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Average Loss: -0.9031, LR: 0.004775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9/10: 100%|██████████| 1524/1524 [28:37<00:00,  1.13s/it, Iter=13710, Loss=-0.9147, Avg Loss=-0.9064, LR=0.004775]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Average Loss: -0.9064, LR: 0.001224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/10: 100%|██████████| 1524/1524 [28:44<00:00,  1.13s/it, Iter=15230, Loss=-0.9133, Avg Loss=-0.9083, LR=0.001224]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Average Loss: -0.9083, LR: 0.000000\n",
      "Checkpoint saved at epoch 10\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# TensorBoard writer\n",
    "writer = SummaryWriter(log_dir=os.path.join(config['save_dir'], config['run']))\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, epoch, config, writer=None):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    total_steps = len(dataloader)\n",
    "    pbar = tqdm(enumerate(dataloader), total=total_steps, desc=f'Epoch {epoch+1}/{config[\"epochs\"]}')\n",
    "    \n",
    "    for batch_idx, (view1, view2) in pbar:\n",
    "        global_step = epoch * total_steps + batch_idx\n",
    "\n",
    "        view1, view2 = view1.to(device), view2.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        p1, p2, z1, z2 = model(view1, view2)\n",
    "\n",
    "        # Loss\n",
    "        loss = simsiam_loss(p1, p2, z1, z2)\n",
    "\n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "            \n",
    "\n",
    "        # Update tqdm and log on tensorboard\n",
    "        if global_step % config['log_interval'] == 0:\n",
    "\n",
    "            pbar.set_postfix({\n",
    "                'Iter': global_step,\n",
    "                'Loss': f'{loss.item():.4f}',\n",
    "                'Avg Loss': f'{total_loss / num_batches:.4f}',\n",
    "                'LR': f'{optimizer.param_groups[0][\"lr\"]:.6f}'\n",
    "            })\n",
    "\n",
    "            writer.add_scalar(\"Train/Batch_Loss\", loss.item(), global_step)\n",
    "\n",
    "    avg_loss = total_loss / num_batches\n",
    "\n",
    "    # Epoch-level logging\n",
    "    if writer:\n",
    "        writer.add_scalar(\"Train/Epoch_Loss\", avg_loss, epoch)\n",
    "        writer.add_scalar(\"Train/Learning_Rate\", optimizer.param_groups[0]['lr'], epoch)\n",
    "\n",
    "    return avg_loss\n",
    "\n",
    "# Training history\n",
    "train_losses = []\n",
    "learning_rates = []\n",
    "\n",
    "print(\"Starting training...\")\n",
    "\n",
    "for epoch in range(config['epochs']):\n",
    "    avg_loss = train_epoch(model, dataloader, optimizer, epoch, config, writer)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    train_losses.append(avg_loss)\n",
    "    learning_rates.append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{config[\"epochs\"]}, Average Loss: {avg_loss:.4f}, LR: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'loss': avg_loss,\n",
    "            'config': config\n",
    "        }\n",
    "        torch.save(checkpoint, os.path.join(config['save_dir'], f'simsiam_epoch_{epoch+1}.pth'))\n",
    "        print(f'Checkpoint saved at epoch {epoch+1}')\n",
    "\n",
    "# Close TensorBoard writer\n",
    "writer.close()\n",
    "print(\"Training completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115dd9ba",
   "metadata": {},
   "source": [
    "### Section 10: Save Final Model and Results\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f430c582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model saved!\n",
      "Encoder saved for downstream tasks!\n"
     ]
    }
   ],
   "source": [
    "# Save final model\n",
    "final_checkpoint = {\n",
    "    'epoch': config['epochs'],\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'scheduler_state_dict': scheduler.state_dict(),\n",
    "    'train_losses': train_losses,\n",
    "    'learning_rates': learning_rates,\n",
    "    'config': config\n",
    "}\n",
    "\n",
    "torch.save(final_checkpoint, os.path.join(config['save_dir'], 'simsiam_final.pth'))\n",
    "print(\"Final model saved!\")\n",
    "\n",
    "# Save encoder only (for downstream tasks)\n",
    "encoder_checkpoint = {\n",
    "    'encoder_state_dict': model.encoder.state_dict(),\n",
    "    'config': config\n",
    "}\n",
    "torch.save(encoder_checkpoint, os.path.join(config['save_dir'], 'simsiam_encoder.pth'))\n",
    "print(\"Encoder saved for downstream tasks!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4484dcb3",
   "metadata": {},
   "source": [
    "### Section 11: Model Evaluation and Feature Extraction\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2838b481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features for analysis...\n",
      "Extracted features shape: (1024, 512)\n",
      "Feature statistics:\n",
      "Mean: 0.5273\n",
      "Std: 0.5048\n",
      "Min: 0.0000\n",
      "Max: 9.0637\n"
     ]
    }
   ],
   "source": [
    "def extract_features(model, dataloader, device, max_samples=1000):\n",
    "    \"\"\"Extract features from the trained encoder\"\"\"\n",
    "    model.eval()\n",
    "    features = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (view1, view2) in enumerate(dataloader):\n",
    "            if batch_idx * dataloader.batch_size >= max_samples:\n",
    "                break\n",
    "                \n",
    "            view1 = view1.to(device)\n",
    "            # Use only first view for feature extraction\n",
    "            feat = model.encoder(view1)\n",
    "            features.append(feat.cpu().numpy())\n",
    "    \n",
    "    return np.concatenate(features, axis=0)\n",
    "\n",
    "# Extract features for analysis\n",
    "print(\"Extracting features for analysis...\")\n",
    "features = extract_features(model, dataloader, device, max_samples=1000)\n",
    "print(f\"Extracted features shape: {features.shape}\")\n",
    "\n",
    "# Analyze feature statistics\n",
    "print(f\"Feature statistics:\")\n",
    "print(f\"Mean: {features.mean():.4f}\")\n",
    "print(f\"Std: {features.std():.4f}\")\n",
    "print(f\"Min: {features.min():.4f}\")\n",
    "print(f\"Max: {features.max():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssl_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
